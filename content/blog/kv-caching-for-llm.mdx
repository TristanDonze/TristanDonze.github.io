---
title: "KV Caching Strategies for Large Language Models"
date: "2024-02-15"
description: "How key-value caches accelerate autoregressive decoding with practical tips for tuning cache size."
cover: "/images/kv-caching-cover.jpg"
---

# KV Caching Strategies for Large Language Models

Key-value (KV) caching stores the intermediate attention keys and values produced at each decoding step so they can be reused instead of recomputed. This drastically reduces the cost of generating long sequences.

The attention scores at token position $t$ are computed as

$$
\text{Attention}(Q_t, K_{\leq t}, V_{\leq t}) = \text{softmax}\left(\frac{Q_t K_{\leq t}^\top}{\sqrt{d_k}}\right) V_{\leq t},
$$

where $Q_t$ is the query for the current token and $K_{\leq t}, V_{\leq t}$ are the cached keys and values for all previous tokens. By reusing $K_{\leq t}$ and $V_{\leq t}$ from the cache, we avoid recomputing the entire prefix at every step.

## Cache Growth

As the sequence grows, so does the cache. In practice, each layer requires storing $2 \times d_{\text{model}} \times L$ values (for keys and values) where $L$ is the generated length. Monitoring memory pressure and applying techniques such as cache chunking or quantization can keep the cache manageable.

## Practical Tips

- Warm your cache with a fixed prompt if you repeatedly query the model with similar instructions.
- Evict cache entries when you switch to a new unrelated conversation to prevent stale context from consuming memory.
- Profile decode latency with and without caching to quantify the actual speedup on your hardware.

KV caching is a straightforward yet impactful optimization that keeps large language models responsive even for lengthy generations.

![KV cache diagram](/images/kv-caching-cover.jpg)
![Attention flow diagram](/images/kv-attention.gif)